---
- hosts: ansible1
  connection: local
  tasks:
    - name: Load AWS vars
      ansible.builtin.include_vars: ../../vars/aws/common.yml
      when: ansible_system_vendor == 'Amazon EC2'
      tags:
        - always

# Run on Ansible-cli host first to avoid CPU contest
- hosts: ansible1:&cis
  become: true
  become_method: ansible.builtin.sudo
  connection: local
  vars:
    ansible_user: "svc_ansible"
    ansible_password: "{{ hostvars['ansible1']['svc_ansible_password'] }}"
    ansible_sudo_pass: "{{ hostvars['ansible1']['svc_ansible_password'] }}"
  tasks:
    - name: Apply AL2 CIS role
      ansible.builtin.include_role:
        name: cis/al2
        public: true
      when:
        - ansible_facts['distribution'] == 'Amazon'
      tags:
        - always

# Note: There is a separate play for each distro.  The reason for this is because all aspects of the role are imported on a play level.
# If multiple include_role tasks are in the same play, you could have RHEL handlers firing off during an Ubuntu role apply, leading to errors.
# This isn't done for the ansible host above because
# it should only ever be a single distribution.  Other instances could be mixed (AL2 on EKS worker nodes, RHEL on Trend DSM)

- hosts:
    - linux:&cis:!ansible1
  strategy: free
  become: true
  become_method: ansible.builtin.sudo
  vars:
    ansible_user: "svc_ansible"
    ansible_password: "{{ hostvars['ansible1']['svc_ansible_password'] }}"
    ansible_sudo_pass: "{{ hostvars['ansible1']['svc_ansible_password'] }}"
  tasks:
    - name: Apply AL2 CIS role
      ansible.builtin.include_role:
        name: cis/al2
        public: true
      when:
        - ansible_facts['distribution'] == 'Amazon'
      tags:
        - always
